{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\project\\bull_ai\\news_aggregator\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from supabase import create_client\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from financial_report_extraction import FinancialContentExtractor\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize your Supabase client here\n",
    "url = os.getenv(\"SUPABASE_URL\")\n",
    "key = os.getenv(\"SUPABASE_KEY\")\n",
    "supabase_client = create_client(url, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_files(bucket: str, path: str):\n",
    "    \"\"\"List all files in a storage bucket path using offset pagination.\"\"\"\n",
    "    all_files = []\n",
    "    offset = 0\n",
    "    limit = 100  # Supabase's max limit\n",
    "    \n",
    "    while True:\n",
    "        # Get batch of files with offset\n",
    "        files = supabase_client.storage.from_(bucket).list(path, {\n",
    "            'limit': limit,\n",
    "            'offset': offset,\n",
    "            'sortBy': {'column': 'name', 'order': 'asc'}\n",
    "        })\n",
    "        \n",
    "        if not files:\n",
    "            break\n",
    "            \n",
    "        all_files.extend(files)\n",
    "        offset += limit\n",
    "        \n",
    "        # Break if we got less than the limit (means we're at the end)\n",
    "        if len(files) < limit:\n",
    "            break\n",
    "            \n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = supabase_client.storage.from_('pdf_chunks').list('test_tables')\n",
    "files = list_all_files('pdf_chunks', 'test_tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2978"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_tables_data(text):\n",
    "    \"\"\"\n",
    "    Extract only sections containing markdown tables and skip sections with \"no table\" messages.\n",
    "    \"\"\"\n",
    "    # Split text by the separator\n",
    "    sections = text.split(\"---------------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    # Skip phrases that indicate no tables\n",
    "    skip_phrases = [\n",
    "        \"no tabular data\",\n",
    "        \"cannot extract tabular\",\n",
    "        \"does not contain any tabular\",\n",
    "        \"no tables\",\n",
    "    ]\n",
    "    \n",
    "    cleaned_sections = []\n",
    "    \n",
    "    for section in sections:\n",
    "        # Skip if section contains any of the skip phrases\n",
    "        if any(phrase in section.lower() for phrase in skip_phrases):\n",
    "            continue\n",
    "            \n",
    "        # Skip if section doesn't contain any markdown tables (checking for |)\n",
    "        if '|' not in section:\n",
    "            continue\n",
    "        \n",
    "        # Add non-empty cleaned section\n",
    "        cleaned_section = section.strip()\n",
    "        if cleaned_section:\n",
    "            cleaned_sections.append(cleaned_section)\n",
    "    \n",
    "    return '\\n\\n' + '='*80 + '\\n\\n'.join(cleaned_sections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0]['name'].endswith('.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    if not file['name'].endswith('.txt'):\n",
    "        continue\n",
    "        \n",
    "    # Download file content\n",
    "    response = supabase_client.storage.from_('pdf_chunks')\\\n",
    "        .download(f'test_tables/{file[\"name\"]}')\n",
    "    \n",
    "    # Clean the content\n",
    "    content = response.decode('utf-8')\n",
    "    cleaned_content = _clean_tables_data(content)\n",
    "    \n",
    "    # Upload cleaned content\n",
    "    supabase_client.storage.from_('pdf_chunks')\\\n",
    "        .update(\n",
    "            f'test_tables/{file[\"name\"]}',\n",
    "            cleaned_content.encode('utf-8'),\n",
    "            {'content-type': 'text/plain'}\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
